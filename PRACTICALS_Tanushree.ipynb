{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-PBuToeNJ6kn"
   },
   "source": [
    "# NATURAL LANGUAGE PROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ew6NeJFJ99U"
   },
   "source": [
    "## EXPT 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "3e273cb5050541ff9b9fc9323273c4b7",
      "181907c81bfe4a09a4f7887d8431abfb",
      "8939481a65574890a770d80bcbabf5ec",
      "a917ca0c8dc24212b37d36809eee114e",
      "f2de86fb90af4058a2fa2b3c771e4579",
      "bc17ee6c639e476f9dc28dfe1889142f",
      "be4ada7063db44c48282c25c3149028f",
      "a7bd2e4edd14442fa59be8c9d33be629",
      "562af2e7238946b4ab9b9660417c9e60",
      "782ae9aab2df4ad2a3066d6e10779fe8",
      "d2f13fb497524409865d5222a8bc2118"
     ]
    },
    "collapsed": true,
    "id": "O9JymEGOIGyR",
    "outputId": "720178aa-02ce-4ed6-c42c-786ad5aa96da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\prajy\\miniconda3\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\prajy\\miniconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\prajy\\miniconda3\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\prajy\\miniconda3\\lib\\site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in c:\\users\\prajy\\miniconda3\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\prajy\\miniconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Requirement already satisfied: spacy in c:\\users\\prajy\\miniconda3\\lib\\site-packages (3.8.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\prajy\\miniconda3\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\prajy\\miniconda3\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\prajy\\miniconda3\\lib\\site-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\prajy\\miniconda3\\lib\\site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\prajy\\miniconda3\\lib\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.0 in c:\\users\\prajy\\miniconda3\\lib\\site-packages (from spacy) (8.3.2)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\prajy\\miniconda3\\lib\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\prajy\\miniconda3\\lib\\site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\prajy\\miniconda3\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\prajy\\miniconda3\\lib\\site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\prajy\\miniconda3\\lib\\site-packages (from spacy) (0.12.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\prajy\\miniconda3\\lib\\site-packages (from spacy) (4.65.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\prajy\\miniconda3\\lib\\site-packages (from spacy) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\prajy\\miniconda3\\lib\\site-packages (from spacy) (2.9.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\prajy\\miniconda3\\lib\\site-packages (from spacy) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\prajy\\miniconda3\\lib\\site-packages (from spacy) (68.2.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\prajy\\miniconda3\\lib\\site-packages (from spacy) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\prajy\\miniconda3\\lib\\site-packages (from spacy) (3.4.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\prajy\\miniconda3\\lib\\site-packages (from spacy) (2.0.2)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\prajy\\miniconda3\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\prajy\\miniconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\prajy\\miniconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.23.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\prajy\\miniconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\prajy\\miniconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\prajy\\miniconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\prajy\\miniconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\prajy\\miniconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
      "Requirement already satisfied: blis<1.1.0,>=1.0.0 in c:\\users\\prajy\\miniconda3\\lib\\site-packages (from thinc<8.4.0,>=8.3.0->spacy) (1.0.1)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\prajy\\miniconda3\\lib\\site-packages (from thinc<8.4.0,>=8.3.0->spacy) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\prajy\\miniconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\prajy\\miniconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\prajy\\miniconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\prajy\\miniconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\prajy\\miniconda3\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\prajy\\miniconda3\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\prajy\\miniconda3\\lib\\site-packages (from jinja2->spacy) (3.0.2)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in c:\\users\\prajy\\miniconda3\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\prajy\\miniconda3\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\prajy\\miniconda3\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\prajy\\miniconda3\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.16.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\prajy\\miniconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
      "^C\n",
      "Requirement already satisfied: transformers in c:\\users\\prajy\\miniconda3\\lib\\site-packages (4.46.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\prajy\\miniconda3\\lib\\site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\prajy\\miniconda3\\lib\\site-packages (from transformers) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\prajy\\miniconda3\\lib\\site-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\prajy\\miniconda3\\lib\\site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\prajy\\miniconda3\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\prajy\\miniconda3\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\users\\prajy\\miniconda3\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\prajy\\miniconda3\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\prajy\\miniconda3\\lib\\site-packages (from transformers) (0.20.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\prajy\\miniconda3\\lib\\site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\prajy\\miniconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\prajy\\miniconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\prajy\\miniconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\prajy\\miniconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\prajy\\miniconda3\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\prajy\\miniconda3\\lib\\site-packages (from requests->transformers) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\prajy\\miniconda3\\lib\\site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\prajy\\miniconda3\\lib\\site-packages (1.5.2)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\prajy\\miniconda3\\lib\\site-packages (from scikit-learn) (2.0.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\prajy\\miniconda3\\lib\\site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\prajy\\miniconda3\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\prajy\\miniconda3\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: stanza in c:\\users\\prajy\\miniconda3\\lib\\site-packages (1.9.2)\n",
      "Requirement already satisfied: emoji in c:\\users\\prajy\\miniconda3\\lib\\site-packages (from stanza) (2.14.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\prajy\\miniconda3\\lib\\site-packages (from stanza) (2.0.2)\n",
      "Requirement already satisfied: protobuf>=3.15.0 in c:\\users\\prajy\\miniconda3\\lib\\site-packages (from stanza) (5.28.3)\n",
      "Requirement already satisfied: requests in c:\\users\\prajy\\miniconda3\\lib\\site-packages (from stanza) (2.31.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\prajy\\miniconda3\\lib\\site-packages (from stanza) (3.4.2)\n",
      "Requirement already satisfied: torch>=1.3.0 in c:\\users\\prajy\\miniconda3\\lib\\site-packages (from stanza) (2.5.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\prajy\\miniconda3\\lib\\site-packages (from stanza) (4.65.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\prajy\\miniconda3\\lib\\site-packages (from torch>=1.3.0->stanza) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\prajy\\miniconda3\\lib\\site-packages (from torch>=1.3.0->stanza) (4.12.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\prajy\\miniconda3\\lib\\site-packages (from torch>=1.3.0->stanza) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\prajy\\miniconda3\\lib\\site-packages (from torch>=1.3.0->stanza) (2024.10.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\prajy\\miniconda3\\lib\\site-packages (from torch>=1.3.0->stanza) (68.2.2)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\prajy\\miniconda3\\lib\\site-packages (from torch>=1.3.0->stanza) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\prajy\\miniconda3\\lib\\site-packages (from sympy==1.13.1->torch>=1.3.0->stanza) (1.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\prajy\\miniconda3\\lib\\site-packages (from requests->stanza) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\prajy\\miniconda3\\lib\\site-packages (from requests->stanza) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\prajy\\miniconda3\\lib\\site-packages (from requests->stanza) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\prajy\\miniconda3\\lib\\site-packages (from requests->stanza) (2024.8.30)\n",
      "Requirement already satisfied: colorama in c:\\users\\prajy\\miniconda3\\lib\\site-packages (from tqdm->stanza) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\prajy\\miniconda3\\lib\\site-packages (from jinja2->torch>=1.3.0->stanza) (3.0.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\prajy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\prajy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\prajy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\prajy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\prajy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch._C'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 17\u001b[0m\n\u001b[0;32m     13\u001b[0m nltk\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maveraged_perceptron_tagger\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mstanza\u001b[39;00m\n\u001b[0;32m     18\u001b[0m stanza\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\prajy\\miniconda3\\Lib\\site-packages\\stanza\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstanza\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpipeline\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DownloadMethod, Pipeline\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstanza\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpipeline\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmultilingual\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MultilingualPipeline\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstanza\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdoc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Document\n",
      "File \u001b[1;32mc:\\Users\\prajy\\miniconda3\\Lib\\site-packages\\stanza\\pipeline\\core.py:18\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstanza\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstant\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m langcode_to_lang\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstanza\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdoc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Document\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstanza\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfoundation_cache\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FoundationCache\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstanza\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m default_device\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstanza\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpipeline\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprocessor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Processor, ProcessorRequirementsException\n",
      "File \u001b[1;32mc:\\Users\\prajy\\miniconda3\\Lib\\site-packages\\stanza\\models\\common\\foundation_cache.py:10\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mthreading\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstanza\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m bert_embedding\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstanza\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchar_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CharacterLanguageModel\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstanza\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpretrain\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Pretrain\n",
      "File \u001b[1;32mc:\\Users\\prajy\\miniconda3\\Lib\\site-packages\\stanza\\models\\common\\bert_embedding.py:5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrnn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pad_packed_sequence, pack_padded_sequence, pack_sequence, PackedSequence\n",
      "File \u001b[1;32mc:\\Users\\prajy\\miniconda3\\Lib\\site-packages\\torch\\nn\\__init__.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# mypy: allow-untyped-defs\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparameter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# usort: skip\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     Buffer \u001b[38;5;28;01mas\u001b[39;00m Buffer,\n\u001b[0;32m      4\u001b[0m     Parameter \u001b[38;5;28;01mas\u001b[39;00m Parameter,\n\u001b[0;32m      5\u001b[0m     UninitializedBuffer \u001b[38;5;28;01mas\u001b[39;00m UninitializedBuffer,\n\u001b[0;32m      6\u001b[0m     UninitializedParameter \u001b[38;5;28;01mas\u001b[39;00m UninitializedParameter,\n\u001b[0;32m      7\u001b[0m )\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodules\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# usort: skip # noqa: F403\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     10\u001b[0m     attention \u001b[38;5;28;01mas\u001b[39;00m attention,\n\u001b[0;32m     11\u001b[0m     functional \u001b[38;5;28;01mas\u001b[39;00m functional,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     16\u001b[0m     utils \u001b[38;5;28;01mas\u001b[39;00m utils,\n\u001b[0;32m     17\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\prajy\\miniconda3\\Lib\\site-packages\\torch\\nn\\parameter.py:4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OrderedDict\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_C\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _disabled_torch_function_impl\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Metaclass to combine _TensorMeta and the instance check override for Parameter.\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01m_ParameterMeta\u001b[39;00m(torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_TensorMeta):\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m# Make `isinstance(t, Parameter)` return True for custom tensor instances that have the _is_param flag.\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch._C'"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "!pip install spacy\n",
    "!pip install transformers\n",
    "!pip install scikit-learn\n",
    "!pip install stanza\n",
    "\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "import spacy\n",
    "\n",
    "import stanza\n",
    "stanza.download('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mYTFyQmNNGrv"
   },
   "source": [
    "## EXPT 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qVmQPdhWPKzz"
   },
   "source": [
    "To study Preprocessing of text\n",
    "\n",
    "\n",
    "*   Tokenization: Uses NLTK to tokenize the text into sentences and words.\n",
    "*   Filtration: Removes punctuation using isalpha() to filter out non-alphabetic characters.\n",
    "*   Script Validation: Checks if each word is valid by ensuring it contains only alphabetic characters.\n",
    "*   Stop Word Removal: Removes common stopwords from the text using NLTK’s built-in English stopword list.\n",
    "*   Stemming: Applies Porter's Stemming Algorithm to reduce words to their root forms.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I7kbVAoDJ2lJ",
    "outputId": "1e869935-2fdd-4d92-c57d-ade88f2f97d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Tokenization ===\n",
      "Word Tokens: ['Natural', 'Language', 'Processing', 'is', 'a', 'sub-field', 'of', 'linguistics', ',', 'computer', 'science', ',', 'and', 'artificial', 'intelligence', '.', 'It', 'is', 'concerned', 'with', 'the', 'interactions', 'between', 'computers', 'and', 'human', 'languages', ',', 'in', 'particular', ',', 'how', 'to', 'program', 'computers', 'to', 'process', 'and', 'analyze', 'large', 'amounts', 'of', 'natural', 'language', 'data', '.']\n",
      "\n",
      "=== Filtration ===\n",
      "Filtered Words (Without Punctuation): ['Natural', 'Language', 'Processing', 'is', 'a', 'of', 'linguistics', 'computer', 'science', 'and', 'artificial', 'intelligence', 'It', 'is', 'concerned', 'with', 'the', 'interactions', 'between', 'computers', 'and', 'human', 'languages', 'in', 'particular', 'how', 'to', 'program', 'computers', 'to', 'process', 'and', 'analyze', 'large', 'amounts', 'of', 'natural', 'language', 'data']\n",
      "\n",
      "=== Script Validation ===\n",
      "Valid Script Words: ['Natural', 'Language', 'Processing', 'is', 'a', 'of', 'linguistics', 'computer', 'science', 'and', 'artificial', 'intelligence', 'It', 'is', 'concerned', 'with', 'the', 'interactions', 'between', 'computers', 'and', 'human', 'languages', 'in', 'particular', 'how', 'to', 'program', 'computers', 'to', 'process', 'and', 'analyze', 'large', 'amounts', 'of', 'natural', 'language', 'data']\n",
      "\n",
      "=== Stop Word Removal ===\n",
      "Text After Stop Word Removal: ['Natural', 'Language', 'Processing', 'linguistics', 'computer', 'science', 'artificial', 'intelligence', 'concerned', 'interactions', 'computers', 'human', 'languages', 'particular', 'program', 'computers', 'process', 'analyze', 'large', 'amounts', 'natural', 'language', 'data']\n",
      "\n",
      "=== Stemming ===\n",
      "Stemmed Words: ['natur', 'languag', 'process', 'linguist', 'comput', 'scienc', 'artifici', 'intellig', 'concern', 'interact', 'comput', 'human', 'languag', 'particular', 'program', 'comput', 'process', 'analyz', 'larg', 'amount', 'natur', 'languag', 'data']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import string\n",
    "\n",
    "text = \"\"\"Natural Language Processing is a sub-field of linguistics, computer science, and artificial intelligence. It is concerned with the interactions between computers and human languages, in particular, how to program computers to process and analyze large amounts of natural language data.\"\"\"\n",
    "\n",
    "print(\"=== Tokenization ===\")\n",
    "words = word_tokenize(text)\n",
    "print(\"Word Tokens:\", words)\n",
    "\n",
    "print(\"\\n=== Filtration ===\")\n",
    "words_filtered = [word for word in words if word.isalpha()]\n",
    "print(\"Filtered Words (Without Punctuation):\", words_filtered)\n",
    "\n",
    "print(\"\\n=== Script Validation ===\")\n",
    "valid_script = [word for word in words_filtered if word.encode().isalpha()]\n",
    "print(\"Valid Script Words:\", valid_script)\n",
    "\n",
    "print(\"\\n=== Stop Word Removal ===\")\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_text = [word for word in valid_script if word.lower() not in stop_words]\n",
    "print(\"Text After Stop Word Removal:\", filtered_text)\n",
    "\n",
    "print(\"\\n=== Stemming ===\")\n",
    "ps = PorterStemmer()\n",
    "stemmed_words = [ps.stem(word) for word in filtered_text]\n",
    "print(\"Stemmed Words:\", stemmed_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v2119sD7TiVI"
   },
   "source": [
    "## EXPT 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uECebzA9T57H"
   },
   "source": [
    "To perform Morphological Analysis\n",
    "\n",
    "* Lemmatized Words: Returns the meaningful root forms, e.g., \"play\" from \"playing\" or \"played.\"\n",
    "* POS Tags: Returns part of speech (like noun, verb, etc.) for each word in the sentence.\n",
    "* Stemmed Words: Returns the base form of words by cutting off suffixes, e.g., \"play\" from \"playing\" or \"cat\" from \"cats.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jRIIttzyOvkA",
    "outputId": "26b4064f-57b2-45c5-b9f2-20f7acfb22b7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sI-8bjDnT9tc",
    "outputId": "e7bff509-e5bb-4653-b4e9-244e27d844ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized Words: ['the', 'cat', 'be', 'play', 'in', 'the', 'garden', '.', 'they', 'have', 'play', 'there', 'before', '.']\n",
      "POS Tags: [('The', 'DT'), ('cats', 'NNS'), ('are', 'VBP'), ('playing', 'VBG'), ('in', 'IN'), ('the', 'DT'), ('garden', 'NN'), ('.', '.'), ('They', 'PRP'), ('have', 'VBP'), ('played', 'VBN'), ('there', 'EX'), ('before', 'IN'), ('.', '.')]\n",
      "Stemmed Words: ['the', 'cat', 'are', 'play', 'in', 'the', 'garden', '.', 'they', 'have', 'play', 'there', 'befor', '.']\n"
     ]
    }
   ],
   "source": [
    "text = \"The cats are playing in the garden. They have played there before.\"\n",
    "\n",
    "def english_lemmatizer(text):\n",
    "    doc = nlp(text)\n",
    "    return [token.lemma_ for token in doc]\n",
    "print(\"Lemmatized Words:\", english_lemmatizer(text))\n",
    "\n",
    "def pos_tagger(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    return nltk.pos_tag(tokens)\n",
    "print(\"POS Tags:\", pos_tagger(text))\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def english_stemmer(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    return [stemmer.stem(token) for token in tokens]\n",
    "print(\"Stemmed Words:\", english_stemmer(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MnJEG4CAWp-z"
   },
   "source": [
    "## EXPT 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R6RFehNZW2KZ"
   },
   "source": [
    "To study N-Gram Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kgCWYZSjUthH",
    "outputId": "f5ad9e37-d71f-4e4b-a538-061b78a182e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BIGRAM\n",
      "P(वह | <s>) = 0.2000\n",
      "P(बहुत | वह) = 0.5000\n",
      "P(खुश | बहुत) = 0.5000\n",
      "P(है। | खुश) = 1.0000\n",
      "P(</s> | है।) = 1.0000\n",
      "TRIGRAM\n",
      "P(वह | <s> <s>) = 0.2000\n",
      "P(बहुत | <s> वह) = 0.5000\n",
      "P(खुश | वह बहुत) = 1.0000\n",
      "P(है। | बहुत खुश) = 1.0000\n",
      "P(</s> | खुश है।) = 1.0000\n",
      "P(</s> | है। </s>) = 1.0000\n"
     ]
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "from collections import defaultdict\n",
    "\n",
    "corpus = [\n",
    "    \"मैं स्कूल जा रहा हूँ।\",\n",
    "    \"वह बहुत खुश है।\",\n",
    "    \"बिल्ली घर के अंदर है।\",\n",
    "    \"हम सब काम कर रहे हैं।\",\n",
    "    \"तुम कहाँ जा रहे हो?\",\n",
    "    \"यहाँ बहुत गर्मी है।\",\n",
    "    \"वह जल्दी आ जाएगा।\",\n",
    "    \"मैं तुम्हें देख सकता हूँ।\",\n",
    "    \"हम मिलेंगे।\",\n",
    "    \"तुम्हें क्या चाहिए?\"\n",
    "]\n",
    "\n",
    "tokenized_sentences = [nltk.word_tokenize(sentence) for sentence in corpus]\n",
    "\n",
    "def calculate_ngram_probabilities(n, tokenized_sentences):\n",
    "    counts = defaultdict(lambda: defaultdict(int))\n",
    "    for sentence in tokenized_sentences:\n",
    "        for ngram in ngrams(sentence, n, pad_left=True, pad_right=True, left_pad_symbol='<s>', right_pad_symbol='</s>'):\n",
    "            counts[ngram[:-1]][ngram[-1]] += 1\n",
    "    return {context: {word: count/sum(words.values()) for word, count in words.items()} for context, words in counts.items()}\n",
    "\n",
    "def display_ngram_probabilities(n, input_sentence, ngram_probs):\n",
    "    tokens = nltk.word_tokenize(input_sentence)\n",
    "    for ngram in ngrams(tokens, n, pad_left=True, pad_right=True, left_pad_symbol='<s>', right_pad_symbol='</s>'):\n",
    "        context, word = ngram[:-1], ngram[-1]\n",
    "        prob = ngram_probs.get(context, {}).get(word, 0)\n",
    "        print(f\"P({word} | {' '.join(context)}) = {prob:.4f}\")\n",
    "\n",
    "\n",
    "bigram_probs = calculate_ngram_probabilities(2, tokenized_sentences)\n",
    "trigram_probs = calculate_ngram_probabilities(3, tokenized_sentences)\n",
    "input_sentence = \"वह बहुत खुश है।\"\n",
    "\n",
    "print(\"BIGRAM\")\n",
    "display_ngram_probabilities(2, input_sentence, bigram_probs)\n",
    "print(\"TRIGRAM\")\n",
    "display_ngram_probabilities(3, input_sentence, trigram_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PapWMZukZHyo"
   },
   "source": [
    "## EXPT 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kw0nZgXuZG77"
   },
   "source": [
    "* NN: Noun, singular or mass\n",
    "* NNP: Proper noun, singular\n",
    "* NST: Noun denoting spatial/temporal\n",
    "* PRP: Personal pronoun\n",
    "* VM: Verb, main\n",
    "* VAUX: Verb, auxiliary\n",
    "* JJ: Adjective\n",
    "* RB: Adverb\n",
    "* PSP: Postposition\n",
    "* QC: Quantifier, cardinal\n",
    "* QF: Quantifier, fractional\n",
    "* RP: Particle\n",
    "* WQ: Wh-word\n",
    "* SYM: Symbol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P7YOW77qYyh1",
    "outputId": "8c18fc20-c453-4f57-8046-3e39a6322ccf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package indian to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/indian.zip.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.tag import UnigramTagger, BigramTagger, DefaultTagger\n",
    "from nltk.corpus import indian\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('indian')\n",
    "nltk.download('punkt')\n",
    "\n",
    "corpus = [\n",
    "    \"मैं स्कूल जा रहा हूँ।\",\n",
    "    \"वह बहुत खुश है।\",\n",
    "    \"बिल्ली घर के अंदर है।\",\n",
    "    \"हम सब काम कर रहे हैं।\",\n",
    "    \"तुम कहाँ जा रहे हो?\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CUPf9JuJZ_cG"
   },
   "outputs": [],
   "source": [
    "hindi_corpus = indian.tagged_sents('hindi.pos')\n",
    "unigram_tagger = UnigramTagger(hindi_corpus, backoff=DefaultTagger('NN'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-hZYr6zDaFSH",
    "outputId": "9f13b3db-9391-4bd5-95a4-89d8f706e706"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: मैं स्कूल जा रहा हूँ।\n",
      "POS Tags: [('मैं', 'PRP'), ('स्कूल', 'NN'), ('जा', 'VAUX'), ('रहा', 'VAUX'), ('हूँ।', 'NN')]\n",
      "\n",
      "\n",
      "Sentence: वह बहुत खुश है।\n",
      "POS Tags: [('वह', 'PRP'), ('बहुत', 'INTF'), ('खुश', 'NN'), ('है।', 'NN')]\n",
      "\n",
      "\n",
      "Sentence: बिल्ली घर के अंदर है।\n",
      "POS Tags: [('बिल्ली', 'NN'), ('घर', 'NN'), ('के', 'PREP'), ('अंदर', 'PREP'), ('है।', 'NN')]\n",
      "\n",
      "\n",
      "Sentence: हम सब काम कर रहे हैं।\n",
      "POS Tags: [('हम', 'PRP'), ('सब', 'INTF'), ('काम', 'NN'), ('कर', 'VFM'), ('रहे', 'VAUX'), ('हैं।', 'NN')]\n",
      "\n",
      "\n",
      "Sentence: तुम कहाँ जा रहे हो?\n",
      "POS Tags: [('तुम', 'NN'), ('कहाँ', 'NN'), ('जा', 'VAUX'), ('रहे', 'VAUX'), ('हो', 'VFM'), ('?', 'PUNC')]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def pos_tagging(sentences):\n",
    "    for sentence in sentences:\n",
    "        tokenized_sentence = word_tokenize(sentence)\n",
    "        tagged_sentence = unigram_tagger.tag(tokenized_sentence)\n",
    "        print(f\"Sentence: {sentence}\")\n",
    "        print(f\"POS Tags: {tagged_sentence}\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "pos_tagging(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h0Qsh_krbCmy"
   },
   "source": [
    "## EXPT 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D3wPxCZ1bFHz"
   },
   "source": [
    "To study chunking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WPjyLJ6naJBz",
    "outputId": "e39e4ea0-a510-4512-c3d3-e8cff0729eb0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package indian to /root/nltk_data...\n",
      "[nltk_data]   Package indian is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.chunk import RegexpParser\n",
    "from nltk.corpus import indian\n",
    "from nltk.tag import UnigramTagger\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('indian')\n",
    "\n",
    "sentences = [\n",
    "    \"मैं स्कूल जा रहा हूँ।\",\n",
    "    \"वह बहुत खुश है।\",\n",
    "    \"बिल्ली घर के अंदर है।\",\n",
    "    \"हम सब काम कर रहे हैं।\",\n",
    "    \"तुम कहाँ जा रहे हो?\",\n",
    "]\n",
    "\n",
    "hindi_corpus = indian.tagged_sents('hindi.pos')\n",
    "unigram_tagger = UnigramTagger(hindi_corpus, backoff=DefaultTagger('NN'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4Lv6heiHbHli",
    "outputId": "d6008465-aa33-497b-9ac1-dc5ba79e7395"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: मैं स्कूल जा रहा हूँ।\n",
      "POS Tags: [('मैं', 'PRP'), ('स्कूल', 'NN'), ('जा', 'VAUX'), ('रहा', 'VAUX'), ('हूँ।', 'NN')]\n",
      "Chunked: (S मैं/PRP (NP स्कूल/NN) जा/VAUX रहा/VAUX (NP हूँ।/NN))\n",
      "\n",
      "Sentence: वह बहुत खुश है।\n",
      "POS Tags: [('वह', 'PRP'), ('बहुत', 'INTF'), ('खुश', 'NN'), ('है।', 'NN')]\n",
      "Chunked: (S वह/PRP बहुत/INTF (NP खुश/NN) (NP है।/NN))\n",
      "\n",
      "Sentence: बिल्ली घर के अंदर है।\n",
      "POS Tags: [('बिल्ली', 'NN'), ('घर', 'NN'), ('के', 'PREP'), ('अंदर', 'PREP'), ('है।', 'NN')]\n",
      "Chunked: (S (NP बिल्ली/NN) (NP घर/NN) के/PREP अंदर/PREP (NP है।/NN))\n",
      "\n",
      "Sentence: हम सब काम कर रहे हैं।\n",
      "POS Tags: [('हम', 'PRP'), ('सब', 'INTF'), ('काम', 'NN'), ('कर', 'VFM'), ('रहे', 'VAUX'), ('हैं।', 'NN')]\n",
      "Chunked: (S हम/PRP सब/INTF (NP काम/NN) कर/VFM रहे/VAUX (NP हैं।/NN))\n",
      "\n",
      "Sentence: तुम कहाँ जा रहे हो?\n",
      "POS Tags: [('तुम', 'NN'), ('कहाँ', 'NN'), ('जा', 'VAUX'), ('रहे', 'VAUX'), ('हो', 'VFM'), ('?', 'PUNC')]\n",
      "Chunked: (S (NP तुम/NN) (NP कहाँ/NN) जा/VAUX रहे/VAUX हो/VFM ?/PUNC)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def pos_tag_and_chunk(sentences):\n",
    "    grammar = r\"\"\"\n",
    "    NP: {<DT>?<JJ>*<NN|NNS>}   # Noun phrase\n",
    "    VP: {<VB.*><NP|PP|CLAUSE>+$}  # Verb phrase\n",
    "    CLAUSE: {<NP><VP>}  # Clause\n",
    "    \"\"\"\n",
    "    chunk_parser = RegexpParser(grammar)\n",
    "\n",
    "    for sentence in sentences:\n",
    "        tokens = word_tokenize(sentence)\n",
    "        tagged = unigram_tagger.tag(tokens)\n",
    "        tree = chunk_parser.parse(tagged)\n",
    "        print(f\"Sentence: {sentence}\\nPOS Tags: {tagged}\\nChunked: {tree}\\n\")\n",
    "\n",
    "pos_tag_and_chunk(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U_drKdUxenoE"
   },
   "source": [
    "## EXPT 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oVHLCwoOepyN"
   },
   "source": [
    "To study NER (Named entity recognition)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ifc-R2UEblwZ",
    "outputId": "1e17bd93-3391-416a-d62e-1ecbe8249573"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'pip' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Using cached https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ip (c:\\Users\\prajy\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (c:\\Users\\prajy\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (c:\\Users\\prajy\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wvi2vhu8exLj",
    "outputId": "1c5eee3d-a5e5-4047-84d5-32b869665e17"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity: England, Label: GPE\n",
      "Entity: 2019, Label: DATE\n",
      "Entity: 2019, Label: DATE\n",
      "Entity: England, Label: GPE\n",
      "Entity: Washington, Label: GPE\n",
      "Entity: US, Label: GPE\n",
      "Entity: first, Label: ORDINAL\n",
      "Entity: US, Label: GPE\n",
      "Entity: Washington, Label: GPE\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "text = \"\"\" England won the 2019 world cup. The 2019 world cup happened in England. Washington is the capital of the US. The first president of the US was Washington.\"\"\"\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(f\"Entity: {ent.text}, Label: {ent.label_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d6qdFJxQe-ua"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "181907c81bfe4a09a4f7887d8431abfb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bc17ee6c639e476f9dc28dfe1889142f",
      "placeholder": "​",
      "style": "IPY_MODEL_be4ada7063db44c48282c25c3149028f",
      "value": "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.9.0.json: "
     }
    },
    "3e273cb5050541ff9b9fc9323273c4b7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_181907c81bfe4a09a4f7887d8431abfb",
       "IPY_MODEL_8939481a65574890a770d80bcbabf5ec",
       "IPY_MODEL_a917ca0c8dc24212b37d36809eee114e"
      ],
      "layout": "IPY_MODEL_f2de86fb90af4058a2fa2b3c771e4579"
     }
    },
    "562af2e7238946b4ab9b9660417c9e60": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "782ae9aab2df4ad2a3066d6e10779fe8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8939481a65574890a770d80bcbabf5ec": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a7bd2e4edd14442fa59be8c9d33be629",
      "max": 48453,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_562af2e7238946b4ab9b9660417c9e60",
      "value": 48453
     }
    },
    "a7bd2e4edd14442fa59be8c9d33be629": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a917ca0c8dc24212b37d36809eee114e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_782ae9aab2df4ad2a3066d6e10779fe8",
      "placeholder": "​",
      "style": "IPY_MODEL_d2f13fb497524409865d5222a8bc2118",
      "value": " 392k/? [00:00&lt;00:00, 16.1MB/s]"
     }
    },
    "bc17ee6c639e476f9dc28dfe1889142f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "be4ada7063db44c48282c25c3149028f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d2f13fb497524409865d5222a8bc2118": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f2de86fb90af4058a2fa2b3c771e4579": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
